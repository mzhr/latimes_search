# Utility functions for accessing and processing files generated by index

import string

def build_stopword(stopwords):
    # Create stopword hashtablie if stopword list given
    stopword_hashtable = {"": ""}
    if stopwords != None:
        # Create stopword hashtable
        for line in stopwords:
            word = line.strip()
            stopword_hashtable[word] = word
    return stopword_hashtable


def remove_stopwords(queries, stopwords):
    # Remove stopwords in line
    for term_no in range(len(queries)-1, -1, -1):
        # Remove the stopword if in the stopwords list
        if queries[term_no] in stopwords:
            del queries[term_no]
    return queries


def build_lexicon(lexicon_file):
    # Process lexicon {word: [invlist position, occurange, total occurance]}
    lexicon = {}
    for line in lexicon_file:
        lexicon_line = line.split();
        lexicon[lexicon_line[0]] = [int(lexicon_line[1]), 
                                    int(lexicon_line[2]), 
                                    int(lexicon_line[3])]
    return lexicon


def build_map(map_file):
    # Process map {documentid: [offset, docno, weight]}
    index_map = {}
    for line in map_file:
        map_line = line.split()
        index_map[int(map_line[0])] = [int(map_line[1]), 
                                       map_line[2], 
                                       int(map_line[3])]
    return index_map


def process_line(line):
    # Process input queries
    for item in range(len(line)):
        line[item] = line[item].translate(None,string.punctuation)
        line[item] = line[item].lower().strip()
    return line
